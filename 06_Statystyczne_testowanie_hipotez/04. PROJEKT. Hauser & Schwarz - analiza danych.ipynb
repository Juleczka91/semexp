{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hauser & Schwarz - analiza danych\n",
    "\n",
    "Wracamy do eksperymentu I z artykułu \"How seemingly innocuous words can bias judgment: Semantic prosody and impression formation\". Będziemy chcieli powtórzyć analizę, którą wykonali autorzy. Mamy już kod do podstawowej obróbki danych, teraz musimy przeprowadzić odpowiednie analizy statystyczne na \"wyczyszczonych\" danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_columns', 50)\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import pingouin as pg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autorzy w swojej analizie obliczyli współczynnik $\\alpha$ Cronbacha. Nie znalazłem odpowiedniej funkcji w żadnej z dostępnych bibliotek, ale nie jest trudno ją zaimplementować. Tę implementację znalazłem w internecie, ale zwraca takie same wyniki jak odpowiednia funkcja w R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CronbachAlpha(itemscores):\n",
    "    itemscores = np.asarray(itemscores)\n",
    "    itemvars = itemscores.var(axis=0, ddof=1)\n",
    "    tscores = itemscores.sum(axis=1)\n",
    "    nitems = itemscores.shape[1]\n",
    "    calpha = nitems / float(nitems-1) * (1 - itemvars.sum() / float(tscores.var(ddof=1)))\n",
    "\n",
    "    return calpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonajmy kod, który napisaliśmy w poprzednim tygodniu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('semantic_prosody_utterly_totally_Daniel.csv')\n",
    "totally = data.dropna(subset = ['tval']).dropna('columns')\n",
    "utterly = data.dropna(subset = ['uval']).dropna('columns')\n",
    "totally.columns = [col[2:] if re.search(string=col, pattern='^._') else col for col in totally.columns]\n",
    "utterly.columns = [col[2:] if re.search(string=col, pattern='^._') else col for col in utterly.columns]\n",
    "totally['adverb'] = 'totally'\n",
    "utterly['adverb'] = 'utterly'\n",
    "totally['val'] = totally['tval']\n",
    "utterly['val'] = utterly['uval']\n",
    "data = pd.concat([totally, utterly], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszą rzeczą, którą zrobili autorzy (o której nie ma nic w artykule) było przeprowadzenie analizy czynnikowej. Ich hipotezą było to, że uda się za jej pomocą wyodrębnić dwa latentne czynniki - jeden dotyczący oceny \"ciepła\" osoby, drugi jej kompetencji. Dla zainteresowanych analizą czynnikową poniżej znajduje się kod wykonujący ją dla zebranych przez Hausera i Schwarza danych. Używa on funkcji `Factor` z modułu `statsmodels.multivariate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['warm',\n",
    "        'gn',\n",
    "        'sincere',\n",
    "        'friendly',\n",
    "        'wi',\n",
    "        'trustworthy',\n",
    "        'competent',\n",
    "        'confident',\n",
    "        'intel',\n",
    "        'capable',\n",
    "        'efficient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td>Eigenvalues</td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td>  <th>warm</th>    <th>gn</th>   <th>sincere</th> <th>friendly</th>   <th>wi</th>   <th>trustworthy</th> <th>competent</th> <th>confident</th>  <th>intel</th> <th>capable</th> <th>efficient</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th> <td>7.5449</td> <td>0.8797</td> <td>0.1664</td>   <td>0.0970</td>  <td>0.0789</td>   <td>0.0624</td>     <td>0.0514</td>    <td>0.0370</td>   <td>0.0249</td> <td>0.0114</td>   <td>-0.0002</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td></td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td>Communality</td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td>  <th>warm</th>    <th>gn</th>   <th>sincere</th> <th>friendly</th>   <th>wi</th>   <th>trustworthy</th> <th>competent</th> <th>confident</th>  <th>intel</th> <th>capable</th> <th>efficient</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th> <td>0.9239</td> <td>0.8691</td> <td>0.8243</td>   <td>0.8753</td>  <td>0.8543</td>   <td>0.8287</td>     <td>0.7992</td>    <td>0.7256</td>   <td>0.6846</td> <td>0.8752</td>   <td>0.6939</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td></td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td>Pre-rotated loadings</td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>       <th>factor 0</th> <th>factor 1</th> <th>factor 2</th> <th>factor 3</th> <th>factor 4</th> <th>factor 5</th> <th>factor 6</th> <th>factor 7</th> <th>factor 8</th> <th>factor 9</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>warm</th>         <td>0.8621</td>   <td>0.3533</td>   <td>0.1920</td>   <td>-0.0963</td>  <td>-0.0013</td>  <td>0.0235</td>   <td>-0.0582</td>  <td>0.0176</td>   <td>0.0681</td>   <td>0.0272</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gn</th>           <td>0.8781</td>   <td>0.2421</td>   <td>0.0842</td>   <td>0.0268</td>   <td>0.1317</td>   <td>-0.0008</td>  <td>-0.0838</td>  <td>0.0165</td>   <td>-0.0703</td>  <td>-0.0444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sincere</th>      <td>0.8501</td>   <td>0.1699</td>   <td>-0.2379</td>  <td>-0.0695</td>  <td>-0.0342</td>  <td>0.0187</td>   <td>0.0322</td>   <td>0.0810</td>   <td>0.0154</td>   <td>-0.0444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>friendly</th>     <td>0.8636</td>   <td>0.2864</td>   <td>0.1041</td>   <td>-0.0493</td>  <td>-0.0860</td>  <td>-0.0812</td>  <td>0.1374</td>   <td>-0.0285</td>  <td>-0.0215</td>  <td>-0.0078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wi</th>           <td>0.8679</td>   <td>0.1880</td>   <td>-0.2211</td>  <td>-0.0143</td>  <td>0.0202</td>   <td>-0.0295</td>  <td>-0.0652</td>  <td>-0.0971</td>  <td>0.0280</td>   <td>0.0309</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trustworthy</th>  <td>0.8580</td>   <td>0.1789</td>   <td>-0.0066</td>  <td>0.2241</td>   <td>-0.0226</td>  <td>0.0770</td>   <td>0.0465</td>   <td>-0.0302</td>  <td>-0.0230</td>  <td>0.0170</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>competent</th>    <td>0.7939</td>   <td>-0.3637</td>  <td>0.0697</td>   <td>0.0110</td>   <td>-0.1136</td>  <td>0.1110</td>   <td>-0.0390</td>  <td>-0.0377</td>  <td>0.0405</td>   <td>-0.0431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>confident</th>    <td>0.8091</td>   <td>-0.1949</td>  <td>-0.0113</td>  <td>0.0309</td>   <td>-0.1066</td>  <td>-0.0377</td>  <td>-0.0622</td>  <td>0.1045</td>   <td>-0.0443</td>  <td>0.0474</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel</th>        <td>0.7266</td>   <td>-0.3409</td>  <td>-0.0062</td>  <td>-0.1579</td>  <td>0.0359</td>   <td>0.0654</td>   <td>0.0301</td>   <td>-0.0511</td>  <td>-0.0771</td>  <td>0.0220</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capable</th>      <td>0.8222</td>   <td>-0.4052</td>  <td>0.0320</td>   <td>0.0518</td>   <td>0.0215</td>   <td>-0.1695</td>  <td>-0.0179</td>  <td>-0.0276</td>  <td>0.0253</td>   <td>-0.0207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>efficient</th>    <td>0.7641</td>   <td>-0.2596</td>  <td>0.0033</td>   <td>0.0260</td>   <td>0.1613</td>   <td>0.0372</td>   <td>0.0898</td>   <td>0.0542</td>   <td>0.0560</td>   <td>0.0203</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td></td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td>varimax rotated loadings</td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>       <th>factor 0</th> <th>factor 1</th> <th>factor 2</th> <th>factor 3</th> <th>factor 4</th> <th>factor 5</th> <th>factor 6</th> <th>factor 7</th> <th>factor 8</th> <th>factor 9</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>warm</th>         <td>0.9080</td>   <td>-0.2921</td>  <td>-0.0243</td>  <td>-0.0502</td>  <td>0.0192</td>   <td>0.0724</td>   <td>-0.0341</td>  <td>0.0271</td>   <td>0.0443</td>   <td>0.0387</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gn</th>           <td>0.8253</td>   <td>-0.3729</td>  <td>-0.0979</td>  <td>0.0571</td>   <td>0.0947</td>   <td>-0.0340</td>  <td>-0.1177</td>  <td>0.0095</td>   <td>-0.0982</td>  <td>-0.0489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sincere</th>      <td>0.6973</td>   <td>-0.3774</td>  <td>-0.4240</td>  <td>0.0372</td>   <td>0.0466</td>   <td>0.0342</td>   <td>0.0509</td>   <td>0.0843</td>   <td>-0.0107</td>  <td>-0.0365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>friendly</th>     <td>0.8465</td>   <td>-0.3371</td>  <td>-0.0898</td>  <td>0.0294</td>   <td>-0.0239</td>  <td>-0.0250</td>  <td>0.1855</td>   <td>0.0080</td>   <td>-0.0216</td>  <td>-0.0001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wi</th>           <td>0.7283</td>   <td>-0.3835</td>  <td>-0.3980</td>  <td>0.0691</td>   <td>-0.0005</td>  <td>-0.0371</td>  <td>-0.0504</td>  <td>-0.0859</td>  <td>-0.0081</td>  <td>0.0473</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trustworthy</th>  <td>0.7411</td>   <td>-0.3964</td>  <td>-0.1539</td>  <td>0.3119</td>   <td>0.0363</td>   <td>-0.0005</td>  <td>0.0030</td>   <td>0.0098</td>   <td>-0.0006</td>  <td>-0.0011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>competent</th>    <td>0.3702</td>   <td>-0.7863</td>  <td>-0.0892</td>  <td>0.0929</td>   <td>-0.0524</td>  <td>0.1488</td>   <td>-0.0106</td>  <td>0.0116</td>   <td>0.0365</td>   <td>-0.0288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>confident</th>    <td>0.4740</td>   <td>-0.6588</td>  <td>-0.1691</td>  <td>0.0755</td>   <td>-0.0514</td>  <td>-0.0151</td>  <td>-0.0149</td>  <td>0.1603</td>   <td>-0.0313</td>  <td>0.0531</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel</th>        <td>0.3270</td>   <td>-0.7130</td>  <td>-0.1554</td>  <td>-0.0470</td>  <td>0.0679</td>   <td>0.1207</td>   <td>0.0429</td>   <td>-0.0458</td>  <td>-0.1354</td>  <td>0.0395</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capable</th>      <td>0.3623</td>   <td>-0.8407</td>  <td>-0.1116</td>  <td>0.0332</td>   <td>0.0163</td>   <td>-0.1517</td>  <td>0.0024</td>   <td>-0.0114</td>  <td>0.0103</td>   <td>-0.0103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>efficient</th>    <td>0.4017</td>   <td>-0.6711</td>  <td>-0.1334</td>  <td>0.0724</td>   <td>0.2422</td>   <td>0.0062</td>   <td>0.0148</td>   <td>0.0097</td>   <td>0.0077</td>   <td>0.0091</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                       Factor analysis results\n",
       "=====================================================================================================\n",
       "                              Eigenvalues                                                            \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "        warm    gn   sincere friendly   wi   trustworthy competent confident intel  capable efficient\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "       7.5449 0.8797  0.1664   0.0970 0.0789      0.0624    0.0514    0.0370 0.0249  0.0114   -0.0002\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "                                                                                                     \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "                              Communality                                                            \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "        warm    gn   sincere friendly   wi   trustworthy competent confident intel  capable efficient\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "       0.9239 0.8691  0.8243   0.8753 0.8543      0.8287    0.7992    0.7256 0.6846  0.8752    0.6939\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "                                                                                                     \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "                           Pre-rotated loadings                                                      \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "            factor 0 factor 1 factor 2 factor 3 factor 4 factor 5 factor 6 factor 7 factor 8 factor 9\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "warm          0.8621   0.3533   0.1920  -0.0963  -0.0013   0.0235  -0.0582   0.0176   0.0681   0.0272\n",
       "gn            0.8781   0.2421   0.0842   0.0268   0.1317  -0.0008  -0.0838   0.0165  -0.0703  -0.0444\n",
       "sincere       0.8501   0.1699  -0.2379  -0.0695  -0.0342   0.0187   0.0322   0.0810   0.0154  -0.0444\n",
       "friendly      0.8636   0.2864   0.1041  -0.0493  -0.0860  -0.0812   0.1374  -0.0285  -0.0215  -0.0078\n",
       "wi            0.8679   0.1880  -0.2211  -0.0143   0.0202  -0.0295  -0.0652  -0.0971   0.0280   0.0309\n",
       "trustworthy   0.8580   0.1789  -0.0066   0.2241  -0.0226   0.0770   0.0465  -0.0302  -0.0230   0.0170\n",
       "competent     0.7939  -0.3637   0.0697   0.0110  -0.1136   0.1110  -0.0390  -0.0377   0.0405  -0.0431\n",
       "confident     0.8091  -0.1949  -0.0113   0.0309  -0.1066  -0.0377  -0.0622   0.1045  -0.0443   0.0474\n",
       "intel         0.7266  -0.3409  -0.0062  -0.1579   0.0359   0.0654   0.0301  -0.0511  -0.0771   0.0220\n",
       "capable       0.8222  -0.4052   0.0320   0.0518   0.0215  -0.1695  -0.0179  -0.0276   0.0253  -0.0207\n",
       "efficient     0.7641  -0.2596   0.0033   0.0260   0.1613   0.0372   0.0898   0.0542   0.0560   0.0203\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "                                                                                                     \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "                           varimax rotated loadings                                                  \n",
       "-----------------------------------------------------------------------------------------------------\n",
       "            factor 0 factor 1 factor 2 factor 3 factor 4 factor 5 factor 6 factor 7 factor 8 factor 9\n",
       "-----------------------------------------------------------------------------------------------------\n",
       "warm          0.9080  -0.2921  -0.0243  -0.0502   0.0192   0.0724  -0.0341   0.0271   0.0443   0.0387\n",
       "gn            0.8253  -0.3729  -0.0979   0.0571   0.0947  -0.0340  -0.1177   0.0095  -0.0982  -0.0489\n",
       "sincere       0.6973  -0.3774  -0.4240   0.0372   0.0466   0.0342   0.0509   0.0843  -0.0107  -0.0365\n",
       "friendly      0.8465  -0.3371  -0.0898   0.0294  -0.0239  -0.0250   0.1855   0.0080  -0.0216  -0.0001\n",
       "wi            0.7283  -0.3835  -0.3980   0.0691  -0.0005  -0.0371  -0.0504  -0.0859  -0.0081   0.0473\n",
       "trustworthy   0.7411  -0.3964  -0.1539   0.3119   0.0363  -0.0005   0.0030   0.0098  -0.0006  -0.0011\n",
       "competent     0.3702  -0.7863  -0.0892   0.0929  -0.0524   0.1488  -0.0106   0.0116   0.0365  -0.0288\n",
       "confident     0.4740  -0.6588  -0.1691   0.0755  -0.0514  -0.0151  -0.0149   0.1603  -0.0313   0.0531\n",
       "intel         0.3270  -0.7130  -0.1554  -0.0470   0.0679   0.1207   0.0429  -0.0458  -0.1354   0.0395\n",
       "capable       0.3623  -0.8407  -0.1116   0.0332   0.0163  -0.1517   0.0024  -0.0114   0.0103  -0.0103\n",
       "efficient     0.4017  -0.6711  -0.1334   0.0724   0.2422   0.0062   0.0148   0.0097   0.0077   0.0091\n",
       "=====================================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.multivariate.Factor(data[variables], n_factor=10).fit()\n",
    "model.rotate('varimax')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autorzy skonstruowali dwie skale - \"ciepła\" oraz \"kompetencji\" - i obliczyli współczynnik $\\alpha$ Cronbacha aby ocenić ich wewnętrzną spójność. My również to zrobimy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574364180654292"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CronbachAlpha(data[['warm', 'gn', 'sincere', 'friendly', 'wi', 'trustworthy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935411415916262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CronbachAlpha(data[['competent', 'confident', 'intel', 'capable', 'efficient', 'skillful']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autorzy do celów analiz skonstruowali dwie dodatkowe zmienne - wskaźnik \"ciepła\" oraz \"kompetencji\". Oba są po prostu średnimi z odpowiedzi na odpowiednie pytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean(1) oznacza, że wyciągamy średnią w wierszach tzn. dal każdego uczestnika\n",
    "data['competenceind'] = data[['competent', 'confident', 'intel', 'capable', 'efficient', 'skillful']].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['warmind'] = data[['warm', 'gn', 'sincere', 'friendly', 'wi', 'trustworthy']].mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie aby móc porównywać wartości trzech zmiennych (`competenceind`, `warmind` oraz `val`) autorzy je wystandaryzowali (obliczyli z-score dla każdej obserwacji). Powodem było to, że były na innych skalach (jedne na 5 punktowej, inne na 7 punktowej skali Likerta). My zrobimy to używając funkcji `zscore` z modułu `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zvalence'] = stats.zscore(data['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zcompetenceind'] = stats.zscore(data['competenceind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zwarmind'] = stats.zscore(data['warmind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz nasze dane należy sprowadzić do postaci \"długiej\". Jako zmiennej identyfikującej użyjemy `V1` - tam jest unikatowy identyfikator użytkownika panelu. Zmienne z wartościami to `zwarmind`, `zcompetenceind` oraz `zvalence`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_l = pd.melt(data, \n",
    "                 id_vars= ['adverb', 'V1'], \n",
    "                 value_vars = ['zwarmind', 'zcompetenceind', 'zvalence'],\n",
    "                var_name = 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wreszcie możemy przeprowadzić tak jak w artykule mieszaną analizę wariancji. Do zrobienia tego użyjemy funkcji `mixed_anova` z pakietu `pingouin`. Naszą zmienną wewnątrzgrupową jest `rating` (bo mamy 3 rodzaje ratingu - `zwarmind`, `zcompetenceind` i `zvalence`), a zmienną międzygrupową jest `adverb` (bo każdy badany dostawał tylko jedną z dwóch różniących się przysłówkiem wersji historyjki). Musimy również powiedzieć która kolumna identyfikuje badanego (u nas jest to `V1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>DF1</th>\n",
       "      <th>DF2</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>p-unc</th>\n",
       "      <th>p-GG-corr</th>\n",
       "      <th>np2</th>\n",
       "      <th>eps</th>\n",
       "      <th>sphericity</th>\n",
       "      <th>W-spher</th>\n",
       "      <th>p-spher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adverb</td>\n",
       "      <td>17.845</td>\n",
       "      <td>1</td>\n",
       "      <td>561</td>\n",
       "      <td>17.845</td>\n",
       "      <td>7.543</td>\n",
       "      <td>0.006217</td>\n",
       "      <td>-</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rating</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>1122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>False</td>\n",
       "      <td>0.876</td>\n",
       "      <td>6.95815e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interaction</td>\n",
       "      <td>1.664</td>\n",
       "      <td>2</td>\n",
       "      <td>1122</td>\n",
       "      <td>0.832</td>\n",
       "      <td>2.728</td>\n",
       "      <td>0.065815</td>\n",
       "      <td>-</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source      SS  DF1   DF2      MS      F     p-unc p-GG-corr    np2  \\\n",
       "0       adverb  17.845    1   561  17.845  7.543  0.006217         -  0.013   \n",
       "1       rating   0.000    2  1122   0.000  0.000  1.000000         1  0.000   \n",
       "2  Interaction   1.664    2  1122   0.832  2.728  0.065815         -  0.005   \n",
       "\n",
       "    eps sphericity W-spher      p-spher  \n",
       "0     -          -       -            -  \n",
       "1  0.89      False   0.876  6.95815e-17  \n",
       "2     -          -       -            -  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.mixed_anova(data = data_l, between = 'adverb', within = 'rating', subject = 'V1', dv = 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Łał! Udało nam się otrzymać dokładnie taką samą analizę jak ta wykonana w artykule! Co za przytłaczający sukces!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
